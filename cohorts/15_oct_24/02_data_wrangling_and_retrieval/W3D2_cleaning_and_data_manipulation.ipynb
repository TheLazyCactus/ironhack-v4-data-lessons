{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Cleaning Data](#toc1_)    \n",
    "  - [Null values](#toc1_1_)    \n",
    "    - [What are the different scenarios of missing data?](#toc1_1_1_)    \n",
    "      - [MCAR (Missing Completely at Random)](#toc1_1_1_1_)    \n",
    "      - [MAR (Missing at Random)](#toc1_1_1_2_)    \n",
    "      - [Missing Not At Random (MNAR)](#toc1_1_1_3_)    \n",
    "    - [Why are null values relevant?](#toc1_1_2_)    \n",
    "    - [Cleaning Null Values](#toc1_1_3_)    \n",
    "    - [Checking for Null Values](#toc1_1_4_)    \n",
    "    - [Dropping Null Values](#toc1_1_5_)    \n",
    "    - [Filling Null Values](#toc1_1_6_)    \n",
    "    - [ðŸ’¡ Check for understanding](#toc1_1_7_)    \n",
    "  - [Dealing with Duplicates](#toc1_2_)    \n",
    "    - [Identifying Duplicates](#toc1_2_1_)    \n",
    "    - [Removing Duplicates](#toc1_2_2_)    \n",
    "    - [Removing Duplicates Based on Specific Columns](#toc1_2_3_)    \n",
    "    - [Resetting the Index](#toc1_2_4_)    \n",
    "  - [Formatting Data (Recap)](#toc1_3_)    \n",
    "    - [Formatting Numeric Values (Recap)](#toc1_3_1_)    \n",
    "    - [Formatting Strings (Recap)](#toc1_3_2_)    \n",
    "    - [Formatting Dates](#toc1_3_3_)\n",
    "  - [Changing column datatypes](#toc1_4_)    \n",
    "  - [Cleaning Column Names](#toc1_5_)    \n",
    "- [Using `apply()`, `map()`, and `applymap()`](#toc2_)    \n",
    "    - [More examples](#toc2_1_1_)    \n",
    "      - [Comparing Map and Apply](#toc2_1_1_1_)    \n",
    "      - [Calculating the length of the name](#toc2_1_1_2_)    \n",
    "      - [Converting to float some columns with applymap()](#toc2_1_1_3_)    \n",
    "      - [Modifying columns names with apply()](#toc2_1_1_4_)    \n",
    "    - [ðŸ’¡ Check for understanding](#toc2_1_2_)    \n",
    "    - [ðŸ’¡ Check for understanding](#toc2_1_3_)    \n",
    "- [Filtering Data](#toc3_)    \n",
    "    - [Creating a condition](#toc3_1_1_)    \n",
    "    - [Filtering df](#toc3_1_2_)    \n",
    "    - [Using multiple conditions](#toc3_1_3_)    \n",
    "- [More Data Manipulation](#toc4_)    \n",
    "  - [Setting the index](#toc4_1_)    \n",
    "  - [Adding/removing rows and/or columns](#toc4_2_)    \n",
    "  - [ðŸ’¡ Check for understanding](#toc4_3_)    \n",
    "- [Summary](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Cleaning Data](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Null values](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values (also known as missing values) are common in datasets and can hinder data analysis and modeling. It is essential to handle null values appropriately to ensure accurate and reliable results. Pandas provides various methods to clean and handle null values in datasets.\n",
    "\n",
    "In Python, `None` is a special constant that represents the absence of a value. It is commonly used to indicate that a variable or function has no value or hasn't been assigned any value. For example, if a function does not explicitly return a value, it implicitly returns `None`.\n",
    "\n",
    "On the other hand, `NaN` stands for \"Not a Number\" and is a special value used to represent missing or undefined numerical data. `NaN` is part of the floating-point representation and is commonly used in numeric data structures like Pandas DataFrames and Series to indicate missing or invalid numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[What are the different scenarios of missing data?](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_1_'></a>[MCAR (Missing Completely at Random)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data is missing for reasons that have nothing to do with the information being recorded, i.e. has no connection to any data points in the dataset, whether observed or unobserved. This scenario happens when there's **a glitch in the system collecting the data**, rather than the data itself and there's no way to predict where values will be missing.\n",
    "\n",
    "For example:\n",
    "- **Lost Surveys**: Say a surveyor accidentally dropped completed questionnaires into a lake. The fact that there's information missing from these specific questionnaires doesnâ€™t depend on who filled them out or what they contained. The loss of data is completely random.  \n",
    "\n",
    "- **Sensor Failure in a Weather Station**: A weather station collects temperature data hourly, but sometimes the sensor fails from time to time, for no particular reason. Given these failures donâ€™t depend on the time of day, season, or temperature, the missing values just happen randomly due to a technical issue. However, if the sensor failure was due to temperature, or rainfall, or **any other factor connected to the data being observed**, the data will no longer be MCAR but it would be **MAR**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_2_'></a>[MAR (Missing at Random)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Despite its name, MAR occurs when the absence of data is not random. The probability of missing data is not equal for all measurements. Theyâ€™re more likely for some observations than others. However, measurements of observed variables predict the unequal probability of missing values occurring. ([Statistics by Jim](https://statisticsbyjim.com/basics/missing-data/))\n",
    "\n",
    "This means that we can figure out where a value might be missing based on other information in the dataset.\n",
    "\n",
    "For example: \n",
    "\n",
    "- **Medical Study Missing Data on a Drugâ€™s Side Effects**: In a medical study, older participants might be more likely to drop out or miss follow-up appointments. As a result, we may be missing some follow-up data on side effects for older participants. The missing data is linked to an observable factor (age), but itâ€™s not related to the unobserved side effect outcomes themselves.\n",
    "\n",
    "-  **Student Performance Data**: In a school, students may be more likely to skip reporting their test scores in optional evaluations. For instance, students with lower attendance rates might be more likely to miss submitting their scores. The missing test scores are related to an observed factor (attendance rate), not the missing test scores themselves. \n",
    "\n",
    "- **Workplace Wellness Programs and Job Satisfaction**: Employees who are part-time might be less likely to participate in wellness programs, leading to missing data on wellness program engagement for part-time employees. The missing engagement data is related to observed information (employment status: part-time vs. full-time) rather than how much the part-time employees actually benefit from the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_3_'></a>[Missing Not At Random (MNAR)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNAR data means that the reason for missing data is related to the actual missing data itself, so it's not possible to predict where there will be missing values in the dataset as the information needed to do that is missing.\n",
    "\n",
    "For example:\n",
    "- **Sensitive Information in Surveys**: Suppose a survey asks people about their income, and higher-income individuals are more likely to skip the question due to privacy concerns. Here, the likelihood of a person not responding is directly related to the value of their income (higher income = more likely to skip). This is MNAR because the missing data (income) is missing specifically because of the value it would have been, not because of any other observed variable like age or occupation, although the latter could also be correlated with a high-income.    \n",
    "\n",
    "- **Health Studies and Sensitive Symptoms**: In a medical study, participants with more severe symptoms might skip reporting certain symptoms or drop out of the study because theyâ€™re uncomfortable sharing the extent of their condition. This missing data would be MNAR since people are missing from the dataset specifically because of their symptom severity, which we canâ€™t observe directly once theyâ€™ve dropped out.   \n",
    "\n",
    "- **Employee Feedback and Job Satisfaction**: If a company survey asks employees about job satisfaction, those who are very dissatisfied may skip questions or avoid taking the survey altogether to avoid potential consequences. This situation would be MNAR because the likelihood of missing data (not answering the job satisfaction questions) is directly related to the unobserved values (job dissatisfaction). However, it could be linked to other factors, such as compensation.  \n",
    "\n",
    "- **Credit Scores and Loan Applications**: In a financial study, people with very low credit scores might be less likely to report their scores or apply for loans, creating a gap in data. Since the missing data (credit scores) is related to the unobserved values (the actual low scores), this scenario would be classified as MNAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Why are null values relevant?](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Biased Analysis**: Except for the MCAR case, missing values can bias the conclusion we draw from data, mostly by ignoring a subset of the population that is potentially very different from the population whose data we collected.\n",
    "\n",
    "- **Lower Statistical Power / Precision**: Having missing data reduces the sample size of our dataset, which in turn reduces the precision and power of statistical tests used for association and hypothesis testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### <a id='toc1_1_3_'></a>[Cleaning Null Values](#toc0_)\n",
    "\n",
    "1. Checking for Null Values:\n",
    "   - Use `isnull()` method to check for null values in a DataFrame or Series.\n",
    "   - Use `notnull()` method to check for non-null values in a DataFrame or Series.\n",
    "\n",
    "2. Dropping Null Values:  \n",
    "   _Typically useful for MCAR but can be harmful for MAR since it will likely bias the overall analysis._\n",
    "   - Use `dropna()` method to remove rows with null values from a DataFrame.\n",
    "   - Use `dropna(axis=1)` to remove columns with null values.\n",
    "\n",
    "3. Filling Null Values:\n",
    "   - Use `fillna(value)` method to replace null values with a specific value.\n",
    "   - Use `fillna(method='ffill')` to forward-fill null values with the previous non-null value.\n",
    "   - Use `fillna(method='bfill')` to backward-fill null values with the next non-null value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_4_'></a>[Checking for Null Values](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wfmE3sTTQtEn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset from an online source\n",
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/titanic_train.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtFyKdY5QtEp"
   },
   "outputs": [],
   "source": [
    "# Review dataframe and df columns\n",
    "print(df.columns)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NT--7lkrQtEp"
   },
   "outputs": [],
   "source": [
    "# Checking for Null Values\n",
    "df.isnull()  # Returns a DataFrame with True where values are null\n",
    "# isnull is an alias of isna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWz8OJedQtEr"
   },
   "source": [
    "When working with large datasets, using `isna()` or `isnull()` along with `any()`, `all()` and `sum()` in Pandas becomes essential for quick and efficient data quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJEHelbGQtEs"
   },
   "outputs": [],
   "source": [
    "# Check which cols have any null values\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any column have only null values\n",
    "df.isna().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Yw4qohgQtEt"
   },
   "source": [
    "sum() calculates the sum of each row, considering True as 1 and False as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9V5zJmOyQtEu"
   },
   "outputs": [],
   "source": [
    "# Count the number of null values in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnMRQh_UQtEv"
   },
   "source": [
    "If we add the parameter `axis=1` with the `sum()` function, we can calculate the sum of each row (along the columns) of the DataFrame `df`. This results in a Series that contains the count of null values in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxnMY9G3QtEv"
   },
   "outputs": [],
   "source": [
    "# Get null values per row\n",
    "df.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get % of null values per row\n",
    "df.isna().sum(axis=1) * 2 / df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort descending\n",
    "(df.isna().sum(axis=1) * 2 / df.shape[1]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the null values in each column?\n",
    "round(df.isna().sum() * 100 / df.shape[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_5_'></a>[Dropping Null Values](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34mKoWs0QtEw"
   },
   "outputs": [],
   "source": [
    "# Dropping rows with any Null Values\n",
    "df.dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rows did we remove from our dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uqydYihQtEw"
   },
   "source": [
    "However, as we can see below in the DataFrame, the rows with NaN values have not been removed. To execute the change, it is necessary to use the `inplace=True` option: `df.dropna(inplace=True)` or assign it to a variable such as df = df.dropna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3Fk9iPoQtEw"
   },
   "outputs": [],
   "source": [
    "# Check original dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znhYDZd_QtEx"
   },
   "outputs": [],
   "source": [
    "# Dropping columns with  Null Values\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr9q_dU9QtEx"
   },
   "source": [
    "In the `dropna()` method of Pandas DataFrame, the `subset`, `how`, and `thresh` parameters are used to control the behavior of dropping rows or columns containing NaN (null) values, when we don't want to drop them just because they have *one* null value:\n",
    "\n",
    "- `subset`: It allows you to specify a subset of columns on which to apply the `dropna()` operation. Only the rows containing NaN values in the specified subset of columns will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XF6VCyJFQtEx"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTflgUzPQtEy"
   },
   "outputs": [],
   "source": [
    "# Drop cabin nulls\n",
    "df.dropna(subset=['Cabin']).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq0sJZ8GQtEy"
   },
   "source": [
    "- `how`: It specifies the condition for dropping rows. It can take the values 'any', which means to drop rows containing any NaN values in the `subset`, or 'all', which means to drop rows containing all NaN values in the `subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6T3W08yQtEy"
   },
   "outputs": [],
   "source": [
    "# Drop rows only if ALL values are null\n",
    "df.dropna(how='all').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVBjPHDQQtEz"
   },
   "source": [
    "- `thresh`: It sets a minimum threshold for the number of non-null values that a row must have in the `subset` in order to be kept. Rows with fewer non-null values than the specified threshold will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresh values\n",
    "df.dropna(thresh=3).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_6_'></a>[Filling Null Values](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQnw3UuzQtE0"
   },
   "source": [
    "`fillna()` is a Pandas method used to replace NaN (null) values in a DataFrame or Series with specified values.\n",
    "- You can use `inplace=True` to modify the DataFrame directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMymw3hJQtE0"
   },
   "outputs": [],
   "source": [
    "# Filling Null Values\n",
    "df.fillna(-1).tail()  # Replaces null values with -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXWly2CEQtE0"
   },
   "source": [
    "Careful if we assign a different data type, since Pandas will change the data type of the whole column. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqS92S7eQtE1"
   },
   "outputs": [],
   "source": [
    "# Check dtypes\n",
    "df.dtypes # age is a float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jAByrPiQtE1"
   },
   "outputs": [],
   "source": [
    "# Fill with \"N/A\" instead of -1 \n",
    "df_na = df.fillna(\"N/A\")\n",
    "df_na.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkT-E-r3QtE1"
   },
   "outputs": [],
   "source": [
    "# Check dtypes\n",
    "df_na.dtypes # age is not a float anymore, it's an object now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJBz9ppkQtE2"
   },
   "source": [
    "To avoid this, we can select manually in which column to apply the `fillna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Uk4bfYOQtE2"
   },
   "outputs": [],
   "source": [
    "# Fill Cabin only\n",
    "df.Cabin.fillna(\"N/A\").tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBbuGN-qQtE2"
   },
   "source": [
    "We can also use the mean(), median() etc. to fill the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nAYBD2mQtE2"
   },
   "outputs": [],
   "source": [
    "# Check Age in last rows\n",
    "df.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9-5qhk4QtE2"
   },
   "outputs": [],
   "source": [
    "# Fill with age mean\n",
    "df.Age.fillna(df.Age.mean()).tail() #after filling with the mean, lets see how it would look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2d7rAwLQtE2"
   },
   "source": [
    "- Two common methods for filling NaN values are `ffill`, which forward fills using the last valid value, and `bfill`, which backward fills using the next valid value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhDJIejjQtE3"
   },
   "outputs": [],
   "source": [
    "# Forward-fill null values in the Age column\n",
    "df['Age'].fillna(method='ffill').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward-fill null values in the Age column\n",
    "df['Age'].fillna(method='bfill').tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_7_'></a>[ðŸ’¡ Check for understanding](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKzS4iNfQtE3"
   },
   "source": [
    "Consider the following DataFrame containing information about students:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Cathy', None, 'Eva'],\n",
    "    'Age': [25, 30, None, 22, 28],\n",
    "    'Gender': ['Female', None, 'Female', 'Male', None],\n",
    "    'Score': [90, None, 78, None, 85]\n",
    "}\n",
    "\n",
    "df_students = pd.DataFrame(data)\n",
    "```\n",
    "\n",
    "Your task is to perform the following data cleaning tasks:\n",
    "\n",
    "1. Check for null values in the DataFrame using `isna()` or `isnull()`.\n",
    "\n",
    "2. Replace the null values in the 'Age' column with the average age of the students.\n",
    "\n",
    "3. Replace the null values in the 'Gender' column with \"Female\".\n",
    "\n",
    "4. Drop any rows that have null values in the 'Name' column.\n",
    "\n",
    "5. Forward fill (ffill) the null values in the 'Score' column with the previous valid value.\n",
    "\n",
    "6. After performing all the cleaning steps, print the cleaned DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Dealing with Duplicates](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data analysis, it's common to encounter duplicate values in datasets. Duplicates can distort our analysis and lead to incorrect conclusions. Fortunately, pandas provides efficient methods to handle duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Identifying Duplicates](#toc0_)\n",
    "\n",
    "To identify duplicate rows in a DataFrame, we can use the `duplicated()` method, which returns a boolean Series indicating whether each row is a duplicate or not. We can then use the `sum()` method to count the total number of duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLaGqlLUQtE6"
   },
   "outputs": [],
   "source": [
    "# Check total # of duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AX9Wxf5hQtE6"
   },
   "outputs": [],
   "source": [
    "# Check if there are any duplicates\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Z7XpMOQtE7"
   },
   "source": [
    "To check for duplicates in specific columns, we can use the `duplicated()` method with the `subset` parameter, or just access first to the column and then check with duplicated().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8U2TAOxQtE7"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates in Age column\n",
    "df.duplicated(subset=['Age']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Removing Duplicates](#toc0_)\n",
    "\n",
    "To remove duplicates from a DataFrame, we can use the `drop_duplicates()` method. By default, this method keeps the first occurrence of each duplicated row and removes the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and update the DataFrame\n",
    "df.drop_duplicates(inplace=True) # we know there are none but this is how we would do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Removing Duplicates Based on Specific Columns](#toc0_)\n",
    "\n",
    "Sometimes, we may want to remove duplicates based on specific columns. We can pass a subset of column names to the `drop_duplicates()` method to achieve this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goJamaHDQtE8"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates based on specific columns\n",
    "df.drop_duplicates(subset=['Sex', 'Age']) #lets look at the number of rows if we do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0N2DrAa-QtE8"
   },
   "source": [
    "By default, `drop_duplicates()` keeps the first occurrence of each duplicated row. If we want to keep the last occurrence instead, we can set the `keep` parameter to `'last'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the last occurrence of duplicates\n",
    "df.drop_duplicates(keep='last', inplace=True) # we know there are none but this is how we would do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_4_'></a>[Resetting the Index](#toc0_)\n",
    "\n",
    "When removing duplicates, the DataFrame index may have gaps due to removed rows. To reset the index after removing duplicates, we can use the `reset_index()` method with the `drop=True` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeFIpwkuQtE9"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates and reset the index\n",
    "df_without_duplicates = df.copy()\n",
    "df_without_duplicates = df.drop_duplicates(subset=['Sex', 'Age'])\n",
    "df_without_duplicates.tail() # look at the gaps in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_duplicates.reset_index(drop=True, inplace=True)\n",
    "df_without_duplicates.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Formatting Data (Recap)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Formatting Numeric Values (Recap)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uirIKhtqQtE-"
   },
   "source": [
    "\n",
    "1. `round()` Method:\n",
    "   - Rounds numeric values to a specified number of decimal places.\n",
    "\n",
    "2. `format()` Method:\n",
    "   - Formats numeric values as strings for better representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-SRAQcpGQtE-"
   },
   "outputs": [],
   "source": [
    "num = 123456.78910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_mExpg-QtE-"
   },
   "outputs": [],
   "source": [
    "# Apply round\n",
    "round(num, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiNGpKUbQtE-"
   },
   "outputs": [],
   "source": [
    "# Format to one decimal\n",
    "format(num, '.1f') # .1f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply round to Fare\n",
    "df['Fare'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also use a lambda function:\n",
    "df['Fare'].apply(lambda x: format(x, '.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[Formatting Strings (Recap)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply all of the string methods we learnt about in the data structures, like `len`, `lower`, `upper`, `split`, `replace`, etc. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_example = \"This is my second Pandas Lesson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string length\n",
    "len(str_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name length - direct method\n",
    "df['Name'].len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name length - str method\n",
    "df['Name'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper/lowercase\n",
    "print(str_example.lower())\n",
    "print(str_example.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper/lowercase\n",
    "display(df['Ticket'].str.lower())\n",
    "display(df['Ticket'].str.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_3_'></a>[Changing string values](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ticket'].str.replace('W./C.', 'wc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_4_'></a>[Formatting Dates](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study this in another Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Changing data types](#toc1_4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Convert columns datatypes](#toc1_4_2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting passenger id\n",
    "df['PassengerId'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to boolean\n",
    "df['Survived'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert first Age to string\n",
    "df['Age'] = df['Age'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error with:\n",
    "df['Age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when not sure if it should be float or integer, pd.to_numeric is the best method\n",
    "pd.to_numeric(df['Age'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Cleaning Column Names](#toc1_5_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcw-nQaQQtFE"
   },
   "source": [
    "We can acccess the columns using `df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4dT-BLZQtFE"
   },
   "outputs": [],
   "source": [
    "# Review columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoumDUVkQtFE"
   },
   "source": [
    "In order to modify them, we can assign new column names to `df.columns` by doing `df.columns = [list_of_new_column_names]` or we can use the `rename()` method to just modify a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEkv2BofQtFE"
   },
   "outputs": [],
   "source": [
    "# How can I create a list of columns that is lowercase?\n",
    "df.columns = df.columns.str.lower()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also use a custom list...\n",
    "df.columns = ['passenger_id', 'survived', 'pclass', 'sex', 'age', 'sib_sp', 'par_ch', 'ticket', 'fare',\n",
    "       'cabin', 'embarked']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename using a dictionary\n",
    "renaming_dict = {\n",
    "    'par_ch': 'parents_children',\n",
    "    \"sib_sp\": 'siblings_spouses'\n",
    "}\n",
    "df.rename(columns=renaming_dict, inplace=True)\n",
    "# Alternative\n",
    "# df.rename(renaming_dict, axis=1, inplace=True)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Using `apply()`, `map()`, and `applymap()`](#toc0_)\n",
    "\n",
    "- `apply()`\n",
    "    - Apply a custom function to a Series.\n",
    "    - Useful for element-wise transformations.\n",
    "    - Example: `df['squared_numbers'] = df['numbers'].apply(lambda x: x ** 2)`\n",
    "\n",
    "- `map()`\n",
    "    - Transform Series elements based on a dictionary.\n",
    "    - Replaces elements with corresponding dictionary values.\n",
    "    - Example: `df['gender_mapped'] = df['gender'].map({'M': 'Male', 'F': 'Female'})`\n",
    "\n",
    "- `applymap()`\n",
    "    - Apply a custom function to every element in a DataFrame.\n",
    "    - Useful for element-wise transformations on entire DataFrames.\n",
    "    - Example: `df = df.applymap(lambda x: x.upper())`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcRJSjiLQtFG"
   },
   "outputs": [],
   "source": [
    "# Applying a custom function using apply()\n",
    "def get_yob(age):\n",
    "    return 1912 - age #titanic sank in 1912, we will assume is when Age was recorded\n",
    "\n",
    "# Add YOB to dataset\n",
    "df['yob'] = df['age'].apply(get_yob)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "4CuF1a4_QtFH"
   },
   "outputs": [],
   "source": [
    "# Test out using lambda\n",
    "df['yob'] = df['age'].apply(lambda age: 1912 - age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z39GKWMwQtFI"
   },
   "source": [
    "In the example above, we can see that to create a new column in pandas, we can simply assign a new Series or list to a new column name within the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mWmGSuyQtFI"
   },
   "source": [
    "To edit the information in a whole column in pandas, you can simply assign a new list or array of values to the column you want to modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lz20IAwQtFJ"
   },
   "outputs": [],
   "source": [
    "# Convert the fare from US Dollars to EUR\n",
    "exchange_rate = 0.9877\n",
    "df['fare_eur'] = df['fare'].apply(lambda num: exchange_rate * num)\n",
    "df['fare_eur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply is awesome to place a logic inside of a function and then apply it to our dataframe:\n",
    "\n",
    "def convert_class_description(col):\n",
    "    \"\"\" Converts pclass to ticket class type. \"\"\"\n",
    "    if col == 1:\n",
    "        return 'First Class'\n",
    "    elif col == 2:\n",
    "        return 'Second Class'\n",
    "    else:\n",
    "        return 'Third Class'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pclass'].apply(convert_class_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can even use apply in the whole dataframe to perform some specific logic\n",
    "def join_name_year(row):\n",
    "    return row['name'] + '-' + str(row['age'])\n",
    "\n",
    "\n",
    "df.apply(join_name_year, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL6okbKIQtFJ"
   },
   "outputs": [],
   "source": [
    "# Using map() to transform the 'Sex' column to an integer\n",
    "gender_mapping = {'male': 0, 'female': 1}\n",
    "df['sex_mapped'] = df['sex'].map(gender_mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJhyMk1FQtFJ"
   },
   "outputs": [],
   "source": [
    "# Switch to lambda\n",
    "df.sex.apply(lambda x: 0 if x==\"male\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `applymap()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7EfLBq-QtFJ"
   },
   "outputs": [],
   "source": [
    "# Using applymap() to convert all string columns to uppercase - \n",
    "df = df.applymap(lambda x: x.upper() if isinstance(x, str) else x)\n",
    "\n",
    "# Displaying the modified DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we able to apply the str.upper to the whole dataframe?\n",
    "df.str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[More examples](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_1_'></a>[Comparing Map and Apply](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yI0JDkGQtFK"
   },
   "source": [
    "We have a column called \"Embarked\" containing three possible values: 'C', 'Q', and 'S'. We want to map these values to 0, 1 and 2. In this case, `apply()` with a lambda function would be complex due to the if-elif-else conditions, but `map()` can handle it more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0oJXD0hQtFK"
   },
   "outputs": [],
   "source": [
    "# Mapping 'embarked' values to their full names using map()\n",
    "embarked_mapping = {'C': 0, 'Q': 1, 'S': 2}\n",
    "df['embarked_nr'] = df['embarked'].map(embarked_mapping)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df[['name', 'embarked_nr']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WnDuPoLQtFK"
   },
   "source": [
    "Why is it a float?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu4Hduf0QtFK"
   },
   "outputs": [],
   "source": [
    "# Check null values\n",
    "df.isna().sum() # Because Embarked has null values and it converted it to float to handle NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DCQFgbXQtFK"
   },
   "outputs": [],
   "source": [
    "# Mapping 'Embarked' values to their full names using apply() and a lambda function\n",
    "df['embarked'].apply(lambda x: 0 if x == 'C' else (1 if x == 'Q' else 2))\n",
    "\n",
    "# Note that here it doesn't convert it to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happened with the null values?\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_2_'></a>[Calculating the length of the name](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efltDgD8QtFK"
   },
   "source": [
    "What if we wanted to create a new column with the length of the name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_length'] = df['name'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_3_'></a>[Converting to float some columns with applymap()](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhGUn45mQtFL"
   },
   "source": [
    "Lets look just as an example, how to make float all the following columns: \"PassengerId\", \"Survived\", \"Pclass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"passengerid\", \"survived\", \"pclass\"]].applymap(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_4_'></a>[Modifying columns names with apply()](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I could also use the apply function by converting df.columns to Series\n",
    "pd.Series(df.columns).apply(lambda col: col.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[ðŸ’¡ Check for understanding](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aeuFTc-QtFL"
   },
   "source": [
    "Make the column Embarked_nr as an integer type.\n",
    "\n",
    "- If you get an error, read the error, and think how you should proceed.\n",
    "- If you decide to fill the null values, use the mode() since its a categorical variable.\n",
    "- If you get another error, look at what mode() is returning in order to fix the error and convert to integer the Embarked_nr column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[ðŸ’¡ Check for understanding](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFjykI_pQtFL"
   },
   "source": [
    "You are given a dataset of students' exam scores here: https://raw.githubusercontent.com/data-bootcamp-v4/data/main/student_performance.csv. Your task is to perform the following operations using pandas:\n",
    "\n",
    "1. Read the CSV file into a DataFrame.\n",
    "2. Create a new column \"total_score\" that calculates the total score for each student by summing their \"math score,\" \"reading score,\" and \"writing score.\"\n",
    "3. Create a new column \"grade\" that assigns a grade to each student based on the following criteria:\n",
    "   - If the total score is >= 90, the grade is \"A.\"\n",
    "   - If the total score is >= 80 and < 90, the grade is \"B.\"\n",
    "   - If the total score is >= 70 and < 80, the grade is \"C.\"\n",
    "   - If the total score is >= 60 and < 70, the grade is \"D.\"\n",
    "   - If the total score is < 60, the grade is \"F.\"\n",
    "4. Convert all student names in the \"gender\" column to uppercase.\n",
    "5. Create a new column \"is_passed\" that indicates whether each student has passed the exam or not. If the total score is >= 60, the student has passed; otherwise, they have failed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "eKpY-M-fQtFL"
   },
   "outputs": [],
   "source": [
    "df_students = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/student_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Filtering Data](#toc0_)\n",
    "\n",
    "One of the primary tasks in dataset analysis is filtering rows.\n",
    "\n",
    "When filtering DataFrames in Pandas, you can use boolean indexing to select specific rows based on certain conditions. Here's a step-by-step explanation:\n",
    "\n",
    "1. Identify the column(s) you want to use as a filter condition. For example, in `housing_df` the column named 'SalePrice'.\n",
    "\n",
    "2. Create a condition using a comparison operator (e.g., `>`, `<`, `==`, etc.) and the column(s) you want to filter. For instance, to filter all rows where the 'SalePrice' is greater than 10000, you would use `condition = housing_df['SalePrice'] > 10000`.\n",
    "\n",
    "3. Use the condition to filter the DataFrame. You can do this by passing the condition inside square brackets to the DataFrame. For example, `filtered_df = housing_df[condition]` will create a new DataFrame `filtered_df` containing only the rows where the 'SalePrice' is greater than 10000.\n",
    "\n",
    "Keep in mind that the condition should evaluate to a boolean Series with the same length as the DataFrame, indicating which rows to include (True) or exclude (False).\n",
    "\n",
    "You can also combine multiple conditions using logical operators like `&` for 'and' and `|` for 'or'. For instance, to filter rows where the 'SalePrice' is greater than 10000 and the 'FullBath' is more than 1, you can use `condition = (housing_df['SalePrice'] > 10000) & (housing_df['FullBath'] > 1)`.\n",
    "\n",
    "Filtering allows you to extract specific subsets of data from your DataFrame, making it easier to analyze and work with the data that meets your criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[Creating a condition](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39MxPzRtQtFM"
   },
   "outputs": [],
   "source": [
    "# Check fare col and mean\n",
    "df['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFwZym2_QtFM"
   },
   "outputs": [],
   "source": [
    "df['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filter condition - fares higher than the mean\n",
    "condition = df['fare'] > df['fare'].mean()\n",
    "condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Filtering df](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDMzVRzOQtFM"
   },
   "outputs": [],
   "source": [
    "# Get filtered df\n",
    "filtered_df = df[condition]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it all in one go\n",
    "df[df['fare'] > df['fare'].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[Using multiple conditions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OBJGqyNQtFM"
   },
   "outputs": [],
   "source": [
    "# We can combine boolean operators with filters to add conditions\n",
    "# boolean operators: and is &, or is |\n",
    "\n",
    "# fare higher than mean but still lower than 50\n",
    "df[(df['fare'] > df['fare'].mean()) & (df['fare'] <= 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BUCuF4XQtFN"
   },
   "outputs": [],
   "source": [
    "# To filter on categorical data we can also use .isin()\n",
    "# Get expensive tickets in the lowest classes\n",
    "df[(df['fare'] > 60) & (df['pclass'].isin([2, 3]))]\n",
    "\n",
    "# Alternatively, get cheap tickets in the higher classes\n",
    "df[(df['fare'] < 30) & (df['pclass'].isin([1, 2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use between() for numerical data\n",
    "# Get fares between 90-100\n",
    "df[df['fare'].between(90, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[More Data Manipulation](#toc0_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Setting the index](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PG9jxcpQtFN"
   },
   "source": [
    "To set an index in pandas, you can use the `set_index()` method of the DataFrame. This method allows you to specify which column you want to use as the index for the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically renaming the rows in our df\n",
    "df.set_index('passenger_id',inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Adding/removing rows and/or columns](#toc0_)\n",
    "\n",
    "To add or remove rows and/or columns from a pandas DataFrame, you can use the following methods:\n",
    "\n",
    "1. Adding rows:\n",
    "   - Use the `concat()` method to add rows to the DataFrame.\n",
    "\n",
    "2. Removing rows:\n",
    "   - Use the `drop()` method with the row index or label to remove specific rows.\n",
    "\n",
    "3. Adding columns:\n",
    "   - Using `df[new_column]`, you simply assign a list, Series, or scalar value to the new column name\n",
    "   - Assign a new column to the DataFrame using bracket notation or the `assign()` method.\n",
    "\n",
    "4. Removing columns:\n",
    "   - Use the `drop()` method with the column name and `axis=1` to remove specific columns.\n",
    "   - Alternatively, you can use the `del` keyword to remove a column in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the first row of the df at the end\n",
    "new_df = pd.concat([df, pd.DataFrame(df.iloc[0, :]).T], axis=0)\n",
    "new_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ryy03oAEQtFN"
   },
   "outputs": [],
   "source": [
    "# Remove the row from the new df\n",
    "new_df.drop(1) # This deletes the row with index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc4Z4JLxQtFN"
   },
   "outputs": [],
   "source": [
    "# Remove a column from the dataframe\n",
    "df.drop('name', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a survived_bool col\n",
    "df[\"survived_bool\"] = df['survived'].map({0: False, 1: True})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[ðŸ’¡ Check for understanding](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpVQaMg6QtFP"
   },
   "source": [
    "Use the `supermarket_sales.csv` file for this task.\n",
    "\n",
    "1. **Load the Data**: Use pandas to load the `supermarket_sales.csv` file into a DataFrame.\n",
    "\n",
    "2. **Null Values**: Check if the DataFrame has any null values. If there are any, count the number of null values in each column.\n",
    "\n",
    "5. **Formatting Data**: Round any floating point numbers in the DataFrame to two decimal places.\n",
    "\n",
    "6. **Cleaning Column Names**: Ensure all column names are in lowercase and replace any spaces in the column names with underscores.\n",
    "\n",
    "7. **Using `apply()`, `map()`, and `applymap()`**: Create a new column called 'total_cost' which is the product of the 'quantity' and 'unit_price' columns (assuming these columns exist in your dataset). Use the `apply()` function for this.\n",
    "\n",
    "8. **Filtering Data**: Filter the DataFrame to only include rows where 'total_cost' is greater than the average 'total_cost'.\n",
    "\n",
    "9. **Setting the Index**: Set the 'invoice_id' column (or any other unique identifier) as the index of the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU31I5GSQtFP"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/supermarket_sales.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Summary](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vNdEciwQtFQ"
   },
   "source": [
    "1. Null Values:\n",
    "   - Null values (also known as missing values) can hinder data analysis and modeling.\n",
    "   - Use `isnull()` or `isna()` to check for null values in a DataFrame or Series.\n",
    "   - Use `any()` and `sum()` to efficiently assess data quality.\n",
    "   - Use `dropna()` to remove rows or columns with null values from a DataFrame.\n",
    "   - Parameters like subset, how, and thresh can control the behavior of dropping rows or columns.\n",
    "   - Use `fillna()` to replace null values with specific values, such as `mean()`, `median()`, or forward/backward fill.\n",
    "\n",
    "5. Formatting Data:\n",
    "   - Use `round()` and `format()` to format numeric values.\n",
    "   - Use string methods like `lower()`, `upper()`, `title()`, `strip()`, `split()`, and `replace()`.\n",
    "\n",
    "6. Cleaning Column Names:\n",
    "   - Use df.columns to access column names.\n",
    "   - Modify column names using df.columns or `rename()`.\n",
    "\n",
    "7. Using `apply()`, `map()`, and `applymap()`:\n",
    "   - `apply()`: Applies a custom function to a Series.\n",
    "   - `map()`: Transforms Series elements based on a dictionary.\n",
    "   - `applymap()`: Applies a custom function to every element in a DataFrame.\n",
    "\n",
    "8. Filtering Data:\n",
    "   - Filter rows in a DataFrame using boolean indexing.\n",
    "   - Use comparison operators (<, >, ==) to create conditions.\n",
    "   - Combine multiple conditions using logical operators (& for 'and', | for 'or').\n",
    "\n",
    "9. Setting the Index:\n",
    "   - Use `set_index()` to set an index for the DataFrame.\n",
    "\n",
    "10. Adding/Removing Rows and Columns:\n",
    "   - Use `concat()` to add rows to the DataFrame.\n",
    "   - Use `drop()` with the row index/label to remove specific rows.\n",
    "   - Use bracket notation or `drop()` with axis=1 to add/remove columns."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "537.273px",
    "left": "459.996px",
    "top": "290.824px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
