{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A4vM4q0R9V6"
   },
   "source": [
    "# We are going to write a script that will web scrape the latest data analytics positions from LinkedIn in a Location we choose:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2c20QX4R9V-"
   },
   "source": [
    "####  Download libraries if you haven't done before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rfKehGlIdDw-",
    "outputId": "e12c3b43-1272-4675-a560-9ead6f523081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB 217.9 kB/s eta 0:00:09\n",
      "    --------------------------------------- 0.0/1.8 MB 326.8 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.8 MB 652.2 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.3/1.8 MB 1.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/1.8 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.7/1.8 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.9/1.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.1/1.8 MB 3.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.3/1.8 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.8/1.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 3.3 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "D:\\Data_analyst\\Ironhack\\Anaconda\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from selenium) (2024.6.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sortedcontainers in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.7 MB 3.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/9.7 MB 3.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/9.7 MB 4.2 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/9.7 MB 4.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/9.7 MB 4.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/9.7 MB 4.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.4/9.7 MB 4.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/9.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.8/9.7 MB 4.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.0/9.7 MB 4.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.2/9.7 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.5/9.7 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.8/9.7 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.1/9.7 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.4/9.7 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.7/9.7 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.0/9.7 MB 5.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.3/9.7 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.6/9.7 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.6/9.7 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.3/9.7 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.7/9.7 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/9.7 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.4/9.7 MB 5.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.7/9.7 MB 5.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.2/9.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.7 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.1/9.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.6/9.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.0/9.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.7 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 337.9/481.7 kB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 450.6/481.7 kB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 450.6/481.7 kB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 450.6/481.7 kB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 460.8/481.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 481.7/481.7 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.0/63.0 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 51.2/58.3 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 51.2/58.3 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 51.2/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 343.0 kB/s eta 0:00:00\n",
      "Installing collected packages: h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-24.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.26.1 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
      "Collecting webdriver_manager==4.0.2\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from webdriver_manager==4.0.2) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from webdriver_manager==4.0.2) (0.21.0)\n",
      "Requirement already satisfied: packaging in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from webdriver_manager==4.0.2) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from requests->webdriver_manager==4.0.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from requests->webdriver_manager==4.0.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from requests->webdriver_manager==4.0.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from requests->webdriver_manager==4.0.2) (2024.6.2)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver_manager\n",
      "Successfully installed webdriver_manager-4.0.2\n",
      "Requirement already satisfied: openpyxl in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in d:\\data_analyst\\ironhack\\anaconda\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install selenium\n",
    "!pip install webdriver_manager==4.0.2\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TugFiqYdDxA"
   },
   "source": [
    "####  Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K5fblUwuR9V-"
   },
   "outputs": [],
   "source": [
    "# this is the library that we will use to create break times in order to mimic human behaviour\n",
    "import time\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "# Juicy stuff- these are the Classes we will use for interaction with a webpage:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "# library for interacting with the operating system\n",
    "import os\n",
    "\n",
    "# you know pandas it's your best buddy\n",
    "import pandas as pd\n",
    "\n",
    "# library for directory location:\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "#Ignore warning -- Some methods are going to be deprecated and I didn't change all (mainly in the function scrapper)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyKeDI1TR9WA"
   },
   "source": [
    "#### Webdrivers allows you to use a programming language in designing your test scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I5HHJRnxR9WA"
   },
   "outputs": [],
   "source": [
    "# driver path\n",
    "\n",
    "## driver path should be the location of the folder where the driver you downloaded is ##\n",
    "\n",
    "#driver_path = '/Users/alex/Documents/Tools/Drivers - Selenium/chromedriver'\n",
    "\n",
    "## first we need to initiate the driver - probably the most important part of the code ##\n",
    "#driver = webdriver.Chrome(executable_path = driver_path)\n",
    "\n",
    "# or new version\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "\n",
    "# Mac users with issue with json driver:\n",
    "\n",
    "# First do a backup:\n",
    "# cp ~/.wdm/drivers.json ~/.wdm/drivers_backup.json\n",
    "\n",
    "# Confirm existance of driver.json\n",
    "# ls -l ~/.wdm/drivers.json\n",
    "\n",
    "# Open the file with the below command and manually remove last json key\n",
    "# #sudo nano ~/.wdm/drivers.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hg4HLHuER9WB"
   },
   "outputs": [],
   "source": [
    "# open the website\n",
    "driver.get('https://www.linkedin.com/login/pt?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Twt264TJR9WC"
   },
   "source": [
    "#### First let's do the login to the LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW_9b99dR9WC",
    "outputId": "5f2ae3d8-c004-488e-8258-556885e743da"
   },
   "outputs": [],
   "source": [
    "# login into your LinkedIn account\n",
    "\n",
    "# input your email\n",
    "\n",
    "email = input('Enter your email : ')\n",
    "\n",
    "# here we are going to find the box where we input the email\n",
    "email_box = driver.find_element(By.ID, \"username\") #email_box = driver.find_element_by_id(\"username\") #deprecated\n",
    "\n",
    "#clear the search box if it has already something in there\n",
    "email_box.clear()\n",
    "\n",
    "# with the method send_keys() you can send the information from the script to the webpage\n",
    "email_box.send_keys(email)\n",
    "time.sleep(2)\n",
    "\n",
    "# input the password\n",
    "password = getpass('Enter your password : ')\n",
    "\n",
    "# here we are going to find the box where we input the password\n",
    "pass_box = driver.find_element(By.ID, 'password') #pass_box = driver.find_element_by_id('password') #deprecated\n",
    "\n",
    "#clear the paword box if it has already something in there\n",
    "pass_box.clear()\n",
    "\n",
    "# here we will send the password to the driver\n",
    "pass_box.send_keys(password)\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "login = driver.find_element(By.CLASS_NAME, 'login__form_action_container')\n",
    "login.click()\n",
    "time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxZbpqOSdDxB",
    "outputId": "ad912897-a088-4d3b-cc66-3180465ed778"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    login = driver.find_element(By.CLASS_NAME, 'login__form_action_container')\n",
    "    login.click()\n",
    "    time.sleep(2.5)\n",
    "except:\n",
    "    print(\"Login button already clicked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0aMP9Z4dDxC"
   },
   "outputs": [],
   "source": [
    "#Disable messages tab in case appears inbox messages:\n",
    "try:\n",
    "    remove_message = driver.find_element(By.XPATH, '/html/body/div[5]/div[4]/aside[1]/div[1]/header/div[3]/button')\n",
    "    remove_message.click()\n",
    "\n",
    "except:\n",
    "    try:\n",
    "        remove_message = driver.find_element(By.XPATH, '/html/body/div[6]/div[4]/aside[1]/div[1]/header/div[3]/button')\n",
    "        remove_message.click()\n",
    "\n",
    "    except:\n",
    "        print(\"No messages inbox.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0wlcSlrR9WD"
   },
   "outputs": [],
   "source": [
    "#remeber me on this web browser (if needed )\n",
    "# not_now = driver.find_element_by_class_name('btn__secondary--large-muted')\n",
    "# time.sleep(1)\n",
    "\n",
    "# not_now.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9fPr87mR9WE"
   },
   "source": [
    "#### Let's go to the job icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQEVcYPmR9WE"
   },
   "outputs": [],
   "source": [
    "# search bar\n",
    "\n",
    "# locate the element by the id\n",
    "\n",
    "# or new method\n",
    "job_icon = driver.find_element(By.CSS_SELECTOR, \"span[title='Jobs']\")\n",
    "\n",
    "job_icon.click()\n",
    "time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JqKmBh9R9WF"
   },
   "source": [
    "#### What is the job position you want to search for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aksOK7YbR9WF",
    "outputId": "cbdb1f79-f216-4083-ccd8-6622d42dd955"
   },
   "outputs": [],
   "source": [
    "# this is not the best approach because even with selenium some elements keep on changing (automatically update themselves)\n",
    "# because they are written in\n",
    "# ember.js\n",
    "\n",
    "search_job = driver.find_elements(By.CLASS_NAME,'jobs-search-box__text-input')[0] #search_job = driver.find_elements_by_class_name('jobs-search-box__text-input')[0]\n",
    "\n",
    "job = input('What job do you want to search for: ')\n",
    "search_job.clear()\n",
    "search_job.send_keys(job)\n",
    "time.sleep(2)\n",
    "\n",
    "# switching to cross in jobs\n",
    "search_job.send_keys(Keys.TAB)\n",
    "time.sleep(2)\n",
    "\n",
    "#switching for job location\n",
    "cross_box = driver.switch_to.active_element\n",
    "time.sleep(1)\n",
    "cross_box.send_keys(Keys.TAB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjHVfaipR9WG"
   },
   "source": [
    "#### What is the job location you want to search for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqHdk1eJR9WG",
    "outputId": "b903a1a2-a0bd-47b5-ae54-af46f8edfa99"
   },
   "outputs": [],
   "source": [
    "#Use the Tab key from the keyboard because it allows us to go from the job box to the location box\n",
    "#get the element the cursor is on\n",
    "location_box = driver.switch_to.active_element\n",
    "\n",
    "location = input('Where do you want to search for jobs: ')\n",
    "location_box.send_keys(location)\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM6LkxT5R9WH"
   },
   "source": [
    "#### You can also mimic your keyboard and click different keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oehwUBoFR9WH"
   },
   "outputs": [],
   "source": [
    "# click enter with the parameter keys.ENTER inside the method send_keys()\n",
    "\n",
    "location_box.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCvypbbnR9WI"
   },
   "outputs": [],
   "source": [
    "# Maximize the window\n",
    "#optional:\n",
    "driver.maximize_window()\n",
    "\n",
    "## you can also fullscreen the window\n",
    "\n",
    "#driver.fullscreen_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNjlbShVR9WI"
   },
   "outputs": [],
   "source": [
    "# go to the end of the page in order for all the elements to be loaded\n",
    "## to scroll a section you should find an a tag in that section, click that element and then scroll to the end\n",
    "\n",
    "page = driver.find_element(By.CSS_SELECTOR, \"div[class^='application-outlet']\")\n",
    "page.click()\n",
    "time.sleep(1)\n",
    "\n",
    "job_viewer = driver.find_element(By.CSS_SELECTOR, \"a[class^='disabled ember-view jo']\") #page = driver.find_element_by_css_selector(\"a[class^='disabled ember-view']\")\n",
    "job_viewer.send_keys(Keys.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbBGCCGkR9WI"
   },
   "outputs": [],
   "source": [
    "## build a function ##\n",
    "\n",
    "# GET ALL JOB LINKS\n",
    "\n",
    "''' !!!ADVANCED: talk a little about how to find with partial class name!!!\n",
    "\n",
    "e.g.:\n",
    "      driver.find_elements_by_css_selector(<tagname>\"[<attribute>^='<partial text of the attribute>']\")\n",
    "\n",
    "exammple in the line bellow '''\n",
    "\n",
    "job_raw = driver.find_elements(By.CSS_SELECTOR,\"a[class^='disabled ember-view']\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrapper(numb_j = len(job_raw)): ## add pages\n",
    "\n",
    "    \"\"\" SUMMARY: This function retrieves all the job posts\n",
    "    links from one page and returns a dataset with\n",
    "    the name of the job in one column and the link to the post in the other.\n",
    "    Also it will write the same info in different files\n",
    "    for every single job post.\n",
    "\n",
    "    HOW IT WORKS: Input the number of jobs you want to scrape.\n",
    "    It will search on the page for the elements by css selector\n",
    "    from all the job posts then loop for every single element and retrieve the 'href'.\n",
    "    Also it will click on every job post and find the job name.\n",
    "    This info will be saved in a dictionary that will in the end be converted to\n",
    "    a dataset.\n",
    "    Below we will open and create a text file with the name of the job post\n",
    "    and inside save the link for further details\"\"\"\n",
    "\n",
    "    # For scraper reasons it's required to duplicate the numb_j as it retrieves 2 times the same position:\n",
    "    #numb_j = numb_j*2\n",
    "\n",
    "    # empty list for saving the job names , link and extra info:\n",
    "    job_list = []\n",
    "\n",
    "    # reduce the page size in order to be able to find the name of the job in the right session\n",
    "    # driver.execute_script(\"document.body.style.zoom='67%'\")\n",
    "\n",
    "    # all jobs in the page\n",
    "    job_raw = driver.find_elements(By.CSS_SELECTOR,\"a[class^='disabled ember-view']\")\n",
    "\n",
    "    # go to the end of the page for all the elements to be loaded\n",
    "    page = driver.find_element(By.CSS_SELECTOR,\"a[class^='disabled ember-view']\")\n",
    "    page.send_keys(Keys.END)\n",
    "    # go to the top of the page for all the elements to be loaded\n",
    "    page.send_keys(Keys.CONTROL + Keys.HOME) # combination of the two keys brings you to the top of the element\n",
    "\n",
    "\n",
    "\n",
    "    for j_idx in range(numb_j):\n",
    "        if (j_idx % 2 == 0) | (j_idx % 2 != 0):\n",
    "            # get the job link\n",
    "            ref = job_raw[j_idx].get_attribute('href')\n",
    "            time.sleep(2)\n",
    "\n",
    "            # increase the page size because the inspect for getting the job name where done wiht the page maximized\n",
    "            driver.execute_script(\"document.body.style.zoom='100%'\")\n",
    "\n",
    "            ## let's click on the job post ##\n",
    "            # driver.find_elements_by_css_selector(\"a[class^='disabled ember-view']\")[j_idx].click()\n",
    "            job_raw[j_idx].click()\n",
    "            time.sleep(1.4)\n",
    "\n",
    "            ## then we reduce the page size in order to be able to see the right part of the page\n",
    "            # and find the element with the name of the job ##\n",
    "            driver.execute_script(\"document.body.style.zoom='67%'\")\n",
    "            time.sleep(2.3)\n",
    "\n",
    "            # get the job name with the .text method\n",
    "            job_name = driver.find_element(By.CSS_SELECTOR,\"h1[class^='t-24 t-bold inline']\").text\n",
    "            time.sleep(2.2)\n",
    "#ember616 > span:nth-child(1) > strong\n",
    "\n",
    "            # get job description with css selector + .text method:\n",
    "\n",
    "            company_name = driver.find_element(By.CSS_SELECTOR,\"div[class^='job-details-jobs-unified-top-card__company-name']\").text\n",
    "\n",
    "            # get company name:\n",
    "\n",
    "            job_details = driver.find_element(By.ID,\"job-details\").text\n",
    "\n",
    "            # increase the page size:\n",
    "            driver.execute_script(\"document.body.style.zoom='100%'\")\n",
    "\n",
    "            # populate list:\n",
    "            job_idx_list = [ref, job_name , company_name , job_details]\n",
    "            time.sleep(3)\n",
    "\n",
    "            # page.send_keys(Keys.PAGE_DOWN)\n",
    "\n",
    "            job_list.append(job_idx_list)\n",
    "\n",
    "            print(f\"Collected job : {job_name} for company: {company_name}\")\n",
    "\n",
    "    #Create dataframe:\n",
    "    Job_dataframe = pd.DataFrame(job_list,\n",
    "                                 columns = [\"job_link\", \"position\", \"company name\", \"job description\"]\n",
    "                                ).drop_duplicates()\n",
    "\n",
    "\n",
    "    #Save dataframe in excel file to later use our job\n",
    "    Job_dataframe.to_excel(pathlib.Path().joinpath('scraped_jobs.xlsx'),\n",
    "                           sheet_name='Jobs',\n",
    "                           index= False)\n",
    "\n",
    "    return Job_dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VaiNFGCdDxD",
    "outputId": "78605fd0-c5f9-43e3-a8b4-a71e13c81400",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many job posts do you want to search for:\n",
    "## let's keep it a low number because it takes some time ##\n",
    "jobsN = int(input('How many job posts you want to retrieve: '))\n",
    "\n",
    "scrapper(jobsN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qavuZDw2R9WK"
   },
   "outputs": [],
   "source": [
    "driver.close() # closes the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt299KXyR9WK"
   },
   "source": [
    "### Extra\n",
    "\n",
    "##### Get the html using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMJp1NMoR9WK"
   },
   "outputs": [],
   "source": [
    "# get the page you are in using page_source attribute\n",
    "\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExwvyWE0R9WL",
    "outputId": "d03e58b4-ce1d-4990-a3bc-097dff0d0d5b"
   },
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nI1TqvsyR9WM",
    "outputId": "0b5a9082-0a69-49dc-e78b-9574f743ccb8"
   },
   "outputs": [],
   "source": [
    "# You can now save this one and use it with Beautiful Soup\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# we can mix regex with beautifulsoup in order to find an attribut by its partial name\n",
    "## where we want to find the names of all job positions based on the class attribut and how the class is named in the end\n",
    "## this way we can avoid if the page source changes from time to time. The attributs can change but some parte of it's\n",
    "## name can remain consistent trhought all the pages\n",
    "job_list_dirty = soup.find_all('a', attrs= {'class': re.compile(r'job-card-list__title?')})\n",
    "job_list_clean = [job.text.strip() for job in job_list_dirty]\n",
    "job_list_clean\n",
    "\n",
    "# the same for the company\n",
    "job_company_dirty = soup.find_all('div', attrs={'class': re.compile(r'^artdeco-entity-lockup__subtitle')})\n",
    "job_company_clean = [company.text.strip() for company in job_company_dirty]\n",
    "\n",
    "# make it into a dataset\n",
    "data = zip(job_list_clean, job_company_clean)\n",
    "df = pd.DataFrame(data, columns=['Job', 'Company'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMs76OeBR9WM"
   },
   "outputs": [],
   "source": [
    "#EXTRA\n",
    "# Save cookies in a pickle file\n",
    "import pickle\n",
    "\n",
    "#create an empty folder\n",
    "\n",
    "cookies_dir = 'saved_cookies'\n",
    "lis_dir = os.listdir()\n",
    "\n",
    "if cookies_dir not in lis_dir:\n",
    "    os.mkdir(cookies_dir)\n",
    "else:\n",
    "    pass # os.removedirs(cookies_dir) --> to remove a directory\n",
    "\n",
    "save_location = cookies_dir + '/cookies.pkl'\n",
    "pickle.dump( driver.get_cookies() , open(save_location,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9bSN73CR9WN"
   },
   "outputs": [],
   "source": [
    "# Load cookies\n",
    "\n",
    "cookies = pickle.load(open(save_location, \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbHI1-8dR9WO"
   },
   "outputs": [],
   "source": [
    "## If you want to scroll the job description\n",
    "\n",
    "x = driver.find_elements(By.PARTIAL_LINK_TEXT, 'Retry Premium')[1]\n",
    "x.send_keys(Keys.END)\n",
    "\n",
    "#OR\n",
    "\n",
    "# x = driver.find_element(By.XPATH, \"/html/body/div[7]/div[3]/div[3]/div[2]/div/section[2]/div/div/div[1]/div/div[1]/div/div[2]/div[2]/ul/li[3]/span/a\")\n",
    "# x.send_keys(Keys.END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-A38ycBR9WP",
    "outputId": "6893aaa9-f2e3-4b6c-e7bc-f140be4e26ee"
   },
   "outputs": [],
   "source": [
    "# Input the number of jobs you want to scrap\n",
    "# Save some information about the jobs (title, url, htlm,)\n",
    "\n",
    "\n",
    "job_cards = driver.find_elements(By.CSS_SELECTOR, \"li[class^='jobs-search-results__list-item occludable-update p']\")\n",
    "\n",
    "\n",
    "job_descriptions = driver.find_elements(By.PARTIAL_LINK_TEXT, 'Retry Premium')[1]\n",
    "job_descriptions.send_keys(Keys.END)\n",
    "\n",
    "\n",
    "# for job_card in job_cards:\n",
    "#     time.sleep(2)\n",
    "#     job_card.click()\n",
    "\n",
    "\n",
    "len(job_cards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mmaiSMLR9WP"
   },
   "outputs": [],
   "source": [
    "#### TESTING ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksPh-KmCR9WP"
   },
   "outputs": [],
   "source": [
    "job_cards[0].click()\n",
    "job_descriptions = driver.find_elements(By.PARTIAL_LINK_TEXT, 'Retry Premium for')[1]\n",
    "job_descriptions.send_keys(Keys.END)\n",
    "\n",
    "title = driver.find_element(By.CSS_SELECTOR, \"h2[class^='t-24 t-bold']\").text\n",
    "location = driver.find_element(By.CSS_SELECTOR, \"span[class^='jobs-unified-top-card__bullet']\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV6kVkQcR9WQ"
   },
   "outputs": [],
   "source": [
    "location"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DjHVfaipR9WG",
    "gM6LkxT5R9WH"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
